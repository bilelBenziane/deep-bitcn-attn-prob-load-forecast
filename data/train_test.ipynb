{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2e34375-4d52-414e-ba49-9758d29587d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# command to launch this code ijn the background\n",
    "# nohup python3 train_test_electricity.py > train_test_electricity.log 2>&1 &\n",
    "\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from lib.utils import fix_seed, instantiate_model, read_table, get_emb\n",
    "from lib.train import loop\n",
    "from data.datasets import timeseries_dataset\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "num_cores = 4\n",
    "torch.set_num_threads(2)\n",
    "#%% Initialize parameters for datasets\n",
    "datasets = ['uci_electricity','uci_traffic','kaggle_favorita', 'kaggle_webtraffic', 'kaggle_m5']\n",
    "dim_inputseqlens = [168, 168, 90, 90, 90]\n",
    "dim_outputseqlens = [24, 24, 28, 30, 28]\n",
    "dim_maxseqlens = [500, 500, 150, 150, 119]\n",
    "#%% Initiate experiment\n",
    "dataset_id = 0\n",
    "cuda = 0\n",
    "seed = 0\n",
    "\n",
    "num_samples_train = 1500000 if datasets[dataset_id] == 'kaggle_m5' else 500000\n",
    "num_samples_validate = 30000 if datasets[dataset_id] == 'kaggle_m5' else 10000\n",
    "\n",
    "num_samples_train = 1500000 if datasets[dataset_id] == 'kaggle_m5' else 5000\n",
    "num_samples_validate = 30000 if datasets[dataset_id] == 'kaggle_m5' else 1000\n",
    "\n",
    "\n",
    "num_samples_test = 10000\n",
    "num_samples_test = 1000\n",
    "\n",
    "fix_seed(seed)\n",
    "early_stopping_patience = 5\n",
    "scaling = True\n",
    "epochs = 100\n",
    "#%% Load data\n",
    "dataset_name = datasets[dataset_id]\n",
    "experiment_dir = 'experiments/'+dataset_name\n",
    "dim_inputseqlen = dim_inputseqlens[dataset_id] # Input sequence length\n",
    "dim_outputseqlen = dim_outputseqlens[dataset_id]  # Output prediction length\n",
    "dim_maxseqlen = dim_maxseqlens[dataset_id]\n",
    "# Import data\n",
    "dset = timeseries_dataset(dataset_name, dim_inputseqlen, dim_outputseqlen, dim_maxseqlen)\n",
    "training_set = dset.load('train')\n",
    "validation_set = dset.load('validate')\n",
    "test_set = dset.load('test')\n",
    "\n",
    "# Initialize sample sets\n",
    "id_samples_train = torch.randperm(len(training_set))[:num_samples_train]\n",
    "id_samples_validate = torch.randperm(len(validation_set))[:num_samples_validate]\n",
    "id_samples_test = torch.randperm(len(test_set))[:num_samples_test]\n",
    "\n",
    "#%% Algorithm parameters\n",
    "\n",
    "\n",
    "device = torch.device(cuda)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "file_experiments = experiment_dir + f'/experiments_{dataset_name}.csv'\n",
    "hyperparams_filename = f\"{experiment_dir}/\"\n",
    "d_emb = get_emb(dataset_name)\n",
    "\n",
    "algorithm = 'bitcn_att_skip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c3aa85-d670-4a6d-8035-b73f950b015c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/uci_electricity/bitcn_att_skip/bitcn_att_skip_seed=0_lr=0.001_bs=64_N=6_NATT=4_d_hidden=10_heads=5\n",
      "Epoch 1/100\n",
      "  Train loss: 0.0130 Time: 9.24s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns cannot be a set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_511012/688985465.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m## model train / valid code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m## Train valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_samples_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_validate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_samples_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mloss_validate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mloss_validate_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         torch.save({'epoch':epoch, \n",
      "\u001b[0;32m~/deep-bitcn-attn-prob-load-forecast/lib/train.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, data, optimizer, batch_size, id_samples, train, metrics, scaling)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0myhat_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deep-bitcn-attn-prob-load-forecast/lib/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(yhat, y, quantiles)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NRMSE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ND'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'MAPE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sMAPE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'QuantileLoss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Quantile'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Quantile'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RMSE'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mujta/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;31m# GH47215\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns cannot be a set"
     ]
    }
   ],
   "source": [
    "# main loop to test different version of this architecture\n",
    "time_per_cong=[]\n",
    "for learning_rate in [0.001]:\n",
    "    for batch_size in [64]:\n",
    "         for d_hidden in [10,15,20,25,30]:\n",
    "            start_time = time.time()    \n",
    "             \n",
    "            for seed in  [0,1,2,3,4]:\n",
    "                N = 6\n",
    "                NATT = 4\n",
    "                fix_seed(seed)\n",
    "                dropout = 0.1\n",
    "                kernel_size = 9\n",
    "                heads = 5\n",
    "                \n",
    "                params= [training_set.d_lag, training_set.d_cov, d_emb,training_set.dim_output,d_hidden, dropout, N,kernel_size,NATT,heads]\n",
    "            \n",
    "                ## initi the model\n",
    "                filename = f\"{experiment_dir}/{algorithm}/{algorithm}_seed={seed}_lr={learning_rate}_bs={batch_size}_N={N}_NATT={NATT}_d_hidden={d_hidden}_heads={heads}\"\n",
    "                print(filename)\n",
    "                if not os.path.isdir(f\"{experiment_dir}/{algorithm}\"): os.makedirs(f\"{experiment_dir}/{algorithm}\")\n",
    "                fix_seed(seed)\n",
    "                n_batch_train = (len(id_samples_train) + batch_size - 1) // batch_size \n",
    "                n_batch_validate = (len(id_samples_validate) + batch_size - 1) // batch_size\n",
    "                if 'model' in locals(): del model\n",
    "            \n",
    "                model = instantiate_model(algorithm)(*params).to(device)   \n",
    "            \n",
    "                ############## Train #################\n",
    "                optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "                loss_train = np.zeros((epochs))\n",
    "                loss_validate = np.zeros((epochs))\n",
    "                loss_validate_best = 1e6\n",
    "                early_stopping_counter = 0\n",
    "                best_epoch = 0\n",
    "            \n",
    "                ## model train / valid code \n",
    "                ## Train valid \n",
    "                for epoch in range(epochs):\n",
    "                    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "                    model, loss_train[epoch], _, _, _, _ = loop(model, training_set, optimizer, batch_size, id_samples_train, train=True, metrics=True, scaling=scaling)    \n",
    "                    _, loss_validate[epoch], yhat_tot, y_tot, x_tot, df_validate = loop(model, validation_set, optimizer, batch_size, id_samples_validate, train=False, metrics=True, scaling=scaling)    \n",
    "                    if loss_validate[epoch] < loss_validate_best:\n",
    "                        torch.save({'epoch':epoch, \n",
    "                                   'model_state_dict':model.state_dict(),\n",
    "                                   'optimizer_state_dict':optimizer.state_dict()}, filename)\n",
    "                        df_validate.to_csv(filename + '_validate.csv')\n",
    "                        loss_validate_best = loss_validate[epoch]\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "                    if (early_stopping_counter == early_stopping_patience) | (epoch == epochs - 1):\n",
    "                        loss_train = loss_train / n_batch_train\n",
    "                        loss_validate = loss_validate / n_batch_validate\n",
    "                        df_loss = pd.DataFrame({'Validation_loss':loss_validate,'Training_loss':loss_train})\n",
    "                        df_loss.to_csv(filename + '_loss.csv')\n",
    "                        break\n",
    "             \n",
    "                params= [test_set.d_lag, test_set.d_cov, d_emb,test_set.dim_output,d_hidden, dropout, N,kernel_size,NATT,heads]\n",
    "                filename = f\"{experiment_dir}/{algorithm}/{algorithm}_seed={seed}_lr={learning_rate}_bs={batch_size}_N={N}_NATT={NATT}_d_hidden={d_hidden}_heads={heads}\"\n",
    "            \n",
    "                fix_seed(seed)\n",
    "                n_batch_test = (len(id_samples_test) + batch_size - 1) // batch_size\n",
    "                if 'model' in locals(): del model\n",
    "                model = instantiate_model(algorithm)(*params) \n",
    "            \n",
    "                #print(filename)\n",
    "                checkpoint = torch.load(filename)\n",
    "            \n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model.to(device)\n",
    "                optimizer=None\n",
    "                _, loss_test, yhat_tot, y_tot, x_tot, df_test = loop(model, test_set, optimizer, batch_size, id_samples_test, train=False, metrics=True, scaling=scaling)    \n",
    "                df_test.to_csv(filename + '_test.csv')\n",
    "                        \n",
    "                        \n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            # Convert to hours, minutes, and seconds\n",
    "            hours, rem = divmod(elapsed_time, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            time_per_cong.append(elapsed_time)\n",
    "            print(f\"Training completed in {int(hours)} hours, {int(minutes)} minutes, and {seconds:.2f} seconds.\")                                            \n",
    "                                    \n",
    "\n",
    "with open('electricity_time_per_conf.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(time_per_cong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126234c3-126a-404c-9b54-df173d17dacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mujta)",
   "language": "python",
   "name": "mujta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
